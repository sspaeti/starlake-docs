---
id: intro
slug: /
sidebar_position: 1
sidebar_label: "Overview"
description: "Discover how Starlake revolutionizes data pipelines with its declarative ETL tool for seamless data transformation and orchestration."
---

# Starlake: Open Source Data Integration & ETL Platform

Starlake is an enterprise-grade data pipeline platform that transforms how organizations handle data integration. Using a declarative approach with YAML and SQL, it eliminates complex coding while ensuring robust data governance and quality.

<video width="100%" controls autoPlay loop muted>
  <source src="https://starlake.ai/starlake.webm" type="video/webm" />
  Your browser does not support the video tag.
</video>

## Revolutionize Your Data Workflows

Transform your data engineering with Starlake's innovative approach to pipeline automation. Our platform reduces development time by 80% while ensuring enterprise-grade reliability and scalability.

### Supported Data Sources
Starlake supports a comprehensive range of data sources:

#### File Formats
- Structured: CSV, TSV, PSV (with multi-char separators)
- Semi-structured: JSON, XML, YAML
- Binary: Parquet, Avro, ORC
- Compressed: ZIP, GZIP, BZIP2
- Fixed-width position files
- Excel spreadsheets

#### Cloud Storage
- Amazon S3
- Google Cloud Storage
- Azure Blob Storage
- Local filesystem

#### Databases & Warehouses
- BigQuery
- Snowflake
- Redshift
- Databricks
- PostgreSQL
- MySQL
- Oracle
- SQL Server
- DuckDB

#### Streaming & Events
- Apache Kafka
- Amazon Kinesis
- Google Pub/Sub
- Azure Event Hubs

### Enterprise Features

#### Cloud-Native Integration
- Native support for AWS, GCP, and Azure
- Cloud-agnostic deployment options
- Serverless execution capabilities
- Multi-cloud data synchronization

#### CI/CD Integration
- Git-based version control
- Automated testing with DuckDB
- Pipeline validation workflows
- Infrastructure as Code support
- Integration with:
  - GitHub Actions
  - GitLab CI
  - Azure DevOps
  - Jenkins

#### Security & Governance
- Row-level security
- Column-level encryption
- Data masking and anonymization
- Audit logging and compliance
- GDPR and CCPA support

### Code Less, Deliver More
Break free from traditional ETL complexity. Starlake's declarative approach means you can:
- Build data pipelines using simple YAML configurations
- Transform data with familiar SQL statements
- Automate workflow orchestration without custom scripts
- Deploy production-ready pipelines in minutes, not months

### Complete Data Lifecycle Management
Take control of your entire data journey:
- Automated data extraction from 20+ source types
- Real-time data quality monitoring and validation
- End-to-end data lineage tracking
- Comprehensive audit trails and version control
- Automated schema evolution and compatibility checks

### Enterprise Data Governance
Ensure data quality and compliance at scale:
- Real-time data quality monitoring with customizable rules
- Automated schema enforcement and validation
- Built-in privacy controls and security features
- SLA monitoring and alerting
- Comprehensive audit logging and reporting

### Flexible Deployment Options
Deploy Starlake your way:
- Self-hosted deployment with Docker
- Native cloud integration with AWS, GCP, and Azure
- Local development environment for rapid testing
- Seamless production scaling

## Comprehensive Pipeline Solutions

### No-Code Data Ingestion
Simplify data ingestion with built-in connectors:
- Support for CSV, JSON, XML, and fixed-width formats
- Native integration with major data warehouses
- Real-time streaming support via Kafka
- Automated schema inference and validation
- Built-in error handling and recovery

### Low-Code Transformations
Transform data efficiently using familiar tools:
- Write transformations in standard SQL
- Define business rules in YAML
- Automatic handling of incremental updates
- Built-in support for complex data types
- Version control and change management

### Automated Workflow Orchestration
Let Starlake manage your pipeline complexity:
- Automatic dependency management
- Native integration with Airflow and Dagster
- Visual DAG generation and monitoring
- Parallel execution optimization
- Built-in error handling and recovery

## How Starlake Works

### 1. Extract
Choose from two powerful extraction modes:
- **Native Mode**: Generate optimized database-specific scripts
- **JDBC Mode**: Achieve 1M+ records/second with parallel extraction
- Support for incremental and full loads
- Automated schema evolution handling

### 2. Load
Transform data ingestion into a declarative process:
- Zero-code data loading with YAML configurations
- Automated data quality validation
- Built-in privacy and security controls
- Support for all major data warehouses
- Real-time monitoring and alerting

### 3. Transform
Simplify transformations with SQL and YAML:
- Write standard SQL SELECT statements
- Automatic optimization for your target platform
- Built-in support for incremental processing
- Comprehensive data quality validation
- Version control and rollback capabilities

### 4. Orchestrate
Automate your entire data pipeline:
- Intelligent dependency management
- Integration with enterprise schedulers
- Visual pipeline monitoring
- Efficient parallel execution
- Automated error handling and recovery

## Getting Started

Begin your journey to modern data pipeline automation:

1. Define your data sources using simple YAML
2. Write transformations in familiar SQL
3. Let Starlake handle the orchestration

Visit our [Quick Start Guide](/quickstart) to see how Starlake can transform your data engineering workflow.
