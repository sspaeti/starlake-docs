# Connections

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

Connections are defined in the `connections` section under the root attribute `application`.


The following types of connections are supported:

- [Local File System](#local-file-system)
- [BigQuery](#google-bigquery)
- [Spark / Databricks](#apache-spark--databricks)
- [Snowflake](#snowflake)
- [Redshift](#amazon-redshift)
- [Postgres](#postgres)


## Local File System

The local file system connection is used to read and write files to the local file system.

```yaml
application:
    connections:
    local:
        type: local
```

Files will be stored in the `area` directory under the `datasets` directory.
DÃ©fault values for `area` and `datasets` can be set in the `application` section.

```yaml
application:
    datasets = "{{root}}/datasets" # or set it through the SL_DATASETS environnement variable.
    area:
        pending: "pending" # Location where files of pending load are stored. May be overloaded by the ${SL_AREA_PENDING} environment variable.
        unresolved: "unresolved" # Location where files that do not match any pattern are moved. May be overloaded by the ${SL_AREA_UNRESOLVED} environment variable.
        archive: "archive" # Location where files are moved after they have been processed. May be overloaded by the ${SL_AREA_ARCHIVE} environment variable.
        ingesting: "ingesting" # Location where files are moved while they are being processed. May be overloaded by the ${SL_AREA_INGESTING} environment variable.
        accepted: "accepted" # Location where files are moved after they have been processed and accepted. May be overloaded by the ${SL_AREA_ACCEPTED} environment variable.
        rejected: "rejected" # Location where files are moved after they have been processed and rejected. May be overloaded by the ${SL_AREA_REJECTED} environment variable.
        business: "business" # Location where transform tasks store their result. May be overloaded by the ${SL_AREA_BUSINESS} environment variable.
        replay: "replay" # Location rejected records are stored in their orginial format. May be overloaded by the ${SL_AREA_REPLAY} environment variable.
        hiveDatabase: "${domain}_${area}" # Hive database name. May be overloaded by the ${SL_AREA_HIVE_DATABASE} environment variable.

```

## Google BigQuery

Starlake support native and spark bigquery connections.
<Tabs groupId="bq_connections">
<TabItem label="BigQuery" value="bigquery">

```yaml
application:
  connections:
    bigquery:
      type: "bigquery"
      # Uncomment the line below to use the spark bigquery connector instead of the native one.
      # sparkFormat: "bigquery" 
      options:
        location: "us-central1" # EU or US or ...
        authType: "APPLICATION_DEFAULT"
        authScopes: "https://www.googleapis.com/auth/cloud-platform" # comma separated list of scopes
        # writeMethod: # Only when 'sparkFormat' above is set. "direct" # direct or indirect (indirect is required for certain features see https://github.com/GoogleCloudDataproc/spark-bigquery-connector)
        #authType: SERVICE_ACCOUNT_JSON_KEYFILE
        #jsonKeyfile: "/Users/me/.gcloud/keys/starlake-me.json"
        #authType: "ACCESS_TOKEN"
        #gcpAccessToken: "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"
  accessPolicies: # Required when applying access policies to table columns (Column Level Security)
    apply: true
    location: EU
    taxonomy: RGPD
```

</TabItem>

</Tabs>

## Apache Spark / Databricks

Spark connections are used to read and write data from Spark.

<Tabs groupId="spark_connections">

<TabItem label="Spark Parquet" value="spark-parquet">

```yaml
application:
  connections:
    spark:
      type: "spark"
      options:
        # any spark configuration can be set here
      
``` 

</TabItem>

<TabItem label="Spark Delta" value="spark-delta">

In addition to the connection defined below, please download the following jars: 
- [delta-spark_2.12-VERSION.jar](https://repo1.maven.org/maven2/io/delta/delta-spark_2.12) and place it in the `bin/deps` directory of the starlake directory.
- [delta-storage_2.12-VERSION.jar](https://repo1.maven.org/maven2/io/delta/delta-storage) and place it in the `bin/deps` directory of the starlake directory.


```yaml
application:
  connections:
    spark:
      type: "spark"
      options:
        # any spark configuration can be set here
  spark:
    sql:
      extensions: "io.delta.sql.DeltaSparkSessionExtension"
      catalog:
        spark_catalog: "org.apache.spark.sql.delta.catalog.DeltaCatalog"

``` 

</TabItem>
</Tabs>


## Snowflake


<Tabs groupId="snow_connections">

<TabItem label="Snowflake JDBC" value="snow-jdbc">

```yaml

application:
  connectionRef: {{connection}}
  connections:
    snowflake:
      type: jdbc
      # Uncomment the line below to use the spark snowflake connector instead of the native one.
      # sparkFormat: snowflake 
      options:
        url: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com"
        driver: "net.snowflake.client.jdbc.SnowflakeDriver"
        user: {{SNOWFLAKE_USER}}
        password: {{SNOWFLAKE_PASSWORD}}
        warehouse: {{SNOWFLAKE_WAREHOUSE}}
        db: {{SNOWFLAKE_DB}}
        keep_column_case: "off"
        preActions: "alter session set TIMESTAMP_TYPE_MAPPING = 'TIMESTAMP_LTZ';ALTER SESSION SET QUOTED_IDENTIFIERS_IGNORE_CASE = true"
                sfUrl: "{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure you do not prefix by jdbc:snowflake://. This is done by the snowflaek driver
        #sfUrl: "jdbc:snowflake://{{SNOWFLAKE_ACCOUNT}}.snowflakecomputing.com" # make sure the key is prefixed with 'sf' is 'sparkFormat' is set
        # sfUser: {{SNOWFLAKE_USER}} # make sure the key is prefixed with 'sf' is 'sparkFormat' is set
        # sfPassword: {{SNOWFLAKE_PASSWORD}} # make sure the key is prefixed with 'sf' is 'sparkFormat' is set
        # sfWarehouse: {{SNOWFLAKE_WAREHOUSE}} # make sure the key is prefixed with 'sf' is 'sparkFormat' is set
        # sfDatabase: {{SNOWFLAKE_DB}} # make sure the key is prefixed with 'sf' is 'sparkFormat' is set

      
``` 

</TabItem>

</Tabs>


## Amazon Redshift


<Tabs groupId="redshift_connections">

<TabItem label="Redshift JDBC" value="redshift-jdbc">

```yaml

application:
  connections:
    redshift:
      type: jdbc
      # Uncomment the line below if you are running on top of Spark or Databricks
      # sparkFormat: "io.github.spark_redshift_community.spark.redshift"  # set to "redshift" if running on top of Databricks
      options:
        url: "jdbc:redshift://account.region.redshift.amazonaws.com:5439/database",
        driver: com.amazon.redshift.Driver
        password: "{{REDSHIFT_PASSWORD}}"
        tempdir: "s3a://bucketName/data",
        tempdir_region: "eu-central-1" # required only if running from outside AWS (your laptop ...)
        aws_iam_role: "arn:aws:iam::aws_count_id:role/role_name"
  
``` 

</TabItem>

</Tabs>


## Postgres

The syntax below applies to any JDBC database. The example below is for Postgres.

<Tabs groupId="pg_connections">

<TabItem label="Postgres JDBC" value="postgres-jdbc">

```yaml
application:
  connectionRef: "postgresql"
  connections:
    postgresql:
      type: jdbc
      # Uncomment the line below to use the spark postgres connector instead of the native one.
      # sparkFormat: jdbc 
      options:
        url: "jdbc:postgresql://{{POSTGRES_HOST}}:{{POSTGRES_PORT}}/{{POSTGRES_DATABASE}}"
        driver: "org.postgresql.Driver"
        user: "{{DATABASE_USER}}"
        password: "{{DATABASE_PASSWORD}}"
        quoteIdentifiers: false
```  

</TabItem>

</Tabs>

